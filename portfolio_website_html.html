<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta
      name="description"
      content="Alexandra Wilkinson Data Scientist Portfolio"
    />
    <title>Alexandra Wilkinson</title>
    <link
      href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css"
      rel="stylesheet"
      integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3"
      crossorigin="anonymous"
    />
    <script
      src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"
      integrity="sha384-ka7Sk0Gln4gmtz2MlQnikT1wXgYsOg+OMhuP+IlRH9sENBO0LRn5q+8nbTov4+1p"
      crossorigin="anonymous"
    ></script>
    <link rel="stylesheet" href="style.css" />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Nunito:wght@200;500&display=swap"
      rel="stylesheet"
    />
    <script
      src="https://kit.fontawesome.com/4cc14c8056.js"
      crossorigin="anonymous"
    ></script>
  </head>
  <body>
    <!-- navigation bar -->
    <div class="nav-bar">
      <nav class="navbar navbar-expand-lg navbar-light bg-light fixed-top">
        <div class="container-fluid">
          <a class="navbar-brand" href="#name" title="Homepage">
            <img src="./img/logo.png" alt="" class="logo"
          /></a>
          <button
            class="navbar-toggler"
            type="button"
            data-bs-toggle="collapse"
            data-bs-target="#navbarNav"
            aria-controls="navbarNav"
            aria-expanded="false"
            aria-label="Toggle navigation"
          >
            <span class="navbar-toggler-icon"></span>
          </button>
          <div class="collapse navbar-collapse" id="navbarNav">
            <ul class="navbar-nav">
              <li class="nav-item">
                <a class="nav-link" href="#intro">Intro</a>
              </li>
              <li class="nav-item">
                <a class="nav-link" href="#portfolio">Portfolio</a>
              </li>
              <li class="nav-item">
                <a class="nav-link" href="#contact">Contact</a>
              </li>
            </ul>
          </div>
        </div>
      </nav>
    </div>
    <br />

    <!-- title and header - my name -->

    <div class="masthead" id="name">
      <h1>Alexandra Wilkinson</h1>
      <h3>Data Scientist, based in Bath, England</h3>
    </div>
    <p class="photo">
      Photo by
      <a
        class="photo-link"
        href="https://unsplash.com/@nasa?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash"
        >NASA</a
      >
      on
      <a
        class="photo-link"
        href="https://unsplash.com/photos/photo-of-outer-space-Q1p7bh3SHj8?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash"
        >Unsplash</a
      >
    </p>

    <!-- bio section and photo -->
    <section class="content" id="intro">
      <div class="row">
        <div class="col-sm-6">
          <img class="headshot" src="img/headshot.jpeg" />
        </div>
        <div class="col-sm-6 intro-para">
          <p>
            I am a current student at the University of the West of England
            studying for a MSc in Data Science and due to finish in January
            2024.
          </p>
        </div>
      </div>
    </section>

    <!-- portfolio projects  -->

    <div id="portfolio"></div>
    <br />
    <br />
    <div class="portfolio">
      <h2>Portfolio</h2>
      <p class="projects_subhead">My projects I have been working on‚Ä¶</p>
    </div>

    <!-- first row of projects -->

    <div class="row">
      <div class="col-md-4 projects">
        <a href="#portfolio" onclick="openModal('myModal1')">
          <h4>Forecasting Solar Panel Power Outputs</h4>
          <img class="project-img" src="img/sp_img.jpg" />
        </a>
      </div>
      <!-- open up a modal for more info on each project  -->
      <div id="myModal1" class="modal">
        <!-- Solar panel forecasting -->
        <div class="modal-content">
          <span class="close" onclick="closeModal('myModal1')">&times;</span>
          <div class="modal-scrollable-content">
            <h4>
              Forecasting the Power Outputs of Photovoltaic Cells: a comparison
              of statistical, machine learning and deep learning models
            </h4>
            <p class="gh-txt">
              The full report, code and data can be found on GitHub:
            </p>
            <a
              href="https://github.com/AliWilkinson/Solar_panel_power_forecasting/tree/main"
              target="_blank"
              title="Github"
              class="gt-project-link"
            >
              <i class="fab fa-github"></i>
            </a>
            <p class="skills">
              This project demonstrates my skills using Python to build
              statistical models, machine learning models and deep learning
              models for multivariate time series forecasting and compare them
              using performance metrics and data visualisations.
            </p>
            <hr />
            <div class="project-content">
              <p class="modal-subheading">Abstract</p>
              <p>
                Power outputs from solar panels or photovoltaic (PV) cells have
                previously been forecasted using weather data and data from
                large-scale photovoltaic plants. These models are primarily used
                for short-term forecasting ranging from a few minutes to a few
                days into the future. This project analyses the effects of
                weather features on power output and compares three different
                types of models: SARIMAX, XGBoost and LSTM-RNN, developed using
                UK weather data and power output data from a UK residential
                photovoltaic system. The models are used for longer-term
                forecasting of one-year of power outputs and compared using
                plots, RMSE, MAE and R2 . All weather features in the data were
                found to have significant correlations to the power outputs. The
                best performing model overall was the LSTM-RNN from the RMSE and
                MAE.
              </p>
              <p>
                The aim of this project was to build and evaluate commonly used
                PV power output forecasting techniques and suggest a model for
                multivariate, long-term forecasting of a UK residential PV
                system power output.
              </p>
              <p>
                The technical process followed during this project is outlined
                in Figure 1.
              </p>
              <img class="modal-img" src="img/sp_flow_diagram.jpg" />
              <p class="fig-label">
                Figure 1. Graphical representation of technical process.
              </p>
              <p class="modal-subheading">
                Analyse how the PV system's power output changes with time and
                analyse weather features effects on power output.
              </p>

              <p>
                The distributions for each variable were examined by the density
                plots and box plots shown in Figure 2. The Shapiro-Wilk test for
                normality was used alongside these visualisations to test if the
                target variable of <em>Monthly Power Generation (kWh)</em> had a
                normal distribution. The resulting p-value determined the target
                was not normally distributed.
              </p>
              <img class="dens-bp" src="img/sp_dens_bp.jpg" />
              <p class="fig-label">
                Figure 2 Distributions for each variable shown in the density
                plots and box plots.
              </p>
              <p>
                Therefore, Spearman‚Äôs correlation was used to test the
                correlations between the variables and the target, Figure 3.
                These relationships are plotted in Figure 4 and the Spearman‚Äôs
                correlations and the corresponding p-values are displayed in
                Table 1.
              </p>
              <img class="modal-img" src="img/sp_corr.jpg" />
              <p class="fig-label">
                Figure 3 Heatmap showing Spearman‚Äôs correlation between each
                variable. Included: original weather variables and created
                variable: Mean Monthly Temperature.
              </p>
              <br />
              <img class="modal-img" src="img/sp_scatters.jpg" />
              <p class="fig-label">
                Figure 4 Plots of Monthly Power Generation (kWh) against each
                variable
              </p>
              <br />
              <img class="modal-img" src="img/sp_tab_corr.jpg" />
              <p class="fig-label">
                Table 1 Values of the Spearman‚Äôs Correlation for each variable
                with Monthly Power Generation and the corresponding p-value.
              </p>
              <br />
              <p>
                From the correlations and the p-values, all the variables have
                significant correlations to the target, so are maintained for
                use in the models as exogenous features.
              </p>
              <p>
                Figure 5 shows the time series of the target (Monthly Power
                Generation), along with it‚Äôs rolling mean and rolling standard
                deviation. There appears to be a seasonal pattern as well as a
                downward trend in this data. To establish if the features and
                the target are stationary, each feature was decomposed, this is
                shown in Figure 6. Figure 6a, c, e, g and i show the plotted
                time series and b, d, f, h, and j show the corresponding
                decomposed variables. Again, the target (6a and 6b) shows a
                downward trend and seasonality. The Mean Monthly Temperature
                (6f) may have a slight upward trend. The other exogenous
                features do not show any clear trends. The features Sun Hours
                (6d), Mean Monthly Temperature (6f) and No. Frost Days (6j)
                display clear seasonal components.
              </p>
              <img class="bigger-img" src="img/ts.png" />
              <p class="fig-label">
                Figure 5 Line plot of the original Time Series: Monthly Power
                Generation over time (green). With the rolling mean (blue) and
                the rolling standard deviation (orange)
              </p>
              <br />
              <img class="modal-img" src="img/ts_each_var.png" />
              <p class="fig-label">
                Figure 6 Right: time series for each variable and the target
                variable. Left: decomposition results (trend and seasonality)
                corresponding to the variable on the right.
              </p>
              <br />
              <p>
                Augmented Dickey-Fuller (ADF) tests were also performed on the
                exogenous features, the target, the differenced target, and the
                seasonally differenced target. The results from these showed the
                exogenous features to be stationary due to trend and the target
                to not be stationary. They also showed that differencing the
                target once and seasonally differencing the target once made the
                target stationary.
              </p>
              <p>
                ACF plots were also created for each exogenous feature to view
                autocorrelation. Figure 7a, c and d show clear correlations
                between the lags of the features: Sun Hours, No. Frost Days and
                Mean Monthly Temperature. Strong negative correlations at lag
                six can be seen for all these features and strong positive
                correlations can be seen at lag twelve. This shows the presence
                of a yearly season structure in the data. From Figure 7b and
                Figure 6h there appears to be no seasonal component or trend in
                the Rain mm feature.
              </p>
              <img class="bigger-img" src="img/vars_acf.png" />
              <p class="fig-label">
                Figure 7 ACF plots for each exogenous feature.
              </p>
              <p>
                An ACF plot was also created for the detrended target to view
                how many data points the seasonal component repeats over. A peak
                outside of the blue region, greater than zero, at lag twelve
                means the values have statistically significant correlations to
                the observation twelve data points prior, indicating a yearly
                seasonal component.
              </p>
              <img class="modal-img" src="img/target_acf.png" />
              <p class="fig-label">
                Figure 8 ACF plot for detrended Monthly Power Generation.
              </p>
              <p>
                The data was split into a training set and testing set as shown
                in Figure 9, with the testing set being the final 12 months of
                data.
              </p>
              <img class="modal-img" src="img/split.png" />
              <p class="fig-label">
                Figure 9 The training (blue) and test split of the data
                (orange).
              </p>

              <p class="modal-subheading">
                Incorporate target lags and weather data to build three
                long-term, multivariate forecasting models: statistical, machine
                learning and deep learning.
              </p>
              <p>
                The three types of models were chosen to evaluate a wide range
                of forecasting methods. Lags of the target variable and weather
                data were used to train the models. This was done as previously
                some types of models, such as the XGB, had not been developed
                using previous power outputs, only using weather features.
                Additionally, these models are built for longer term predictions
                as opposed to one day-ahead forecasting. The review by Mellit et
                al. (2020) states few studies focus on long-term forecasting and
                expresses the need for its investigation.
              </p>
              <p>
                The full methods for the development of the models can be found
                in the
                <a
                  href="https://github.com/AliWilkinson/Solar_panel_power_forecasting/tree/main"
                  target="_blank"
                  title="Solar Panel power forecasting report"
                >
                  report on GitHub.</a
                >
              </p>
              <p>
                Pmdarima was used to find the orders of autoregressive and
                moving average parts of the SARIMAX model. A function was
                created to test different values of d and D which represent how
                many times the time series should be differenced or seasonally
                differenced to make it stationary, see Figure 10. The model that
                produced the lowest Akaike‚Äôs Information Criterion value was
                chosen to be the final model. The model was then evaluated using
                statsmodels plot_diagonstics summary functions.
              </p>
              <img class="code-img" src="img/sp_sarimax.png" />
              <p class="fig-label">
                Figure 10 Code for the function to test different values of d
                and D and find the best SARIMAX model.
              </p>
              <p>
                A second SARIMAX model was developed to evaluate if seasonal
                differencing of the exogenous features altered model performance
                or fit. Seasonal differencing was performed on the exogenous
                features that showed seasonality and same process as above was
                followed.
              </p>
              <p>
                Seasonally differencing the exogenous features improved model
                performance and the distribution of residuals. A full evaluation
                can be seen in the
                <a
                  href="https://github.com/AliWilkinson/Solar_panel_power_forecasting/tree/main"
                  target="_blank"
                  title="Solar Panel
                power forecasting report"
                  >report on GitHub.</a
                >
              </p>
              <p>
                The XGBoost models were developed using the XGB library. The
                scikit-learn library was used for hyperparameter tuning and
                performance evaluation.
              </p>
              <p>
                One XGB model was built without prior normalisation of the data
                and another XGB model with normalisation. Feature scales can
                affect the predictions of XGB, so this comparison was performed
                to test the performance of the XGB model with and without
                normalisation.
              </p>
              <p>
                A function was created to frame the data as a supervised
                learning problem as to allow the lag data to be used with the
                XGB model, see Figure 11.
              </p>
              <img class="code-img" src="img/sp_xgb.png" />
              <p class="fig-label">
                Figure 11 Code for the function that takes time series data to
                frame as a supervised learning problem.
              </p>
              <p>
                A Random Search approach was used for hyperparameter tuning of
                the XGB models, see Figure 12.
              </p>
              <img class="code-img" src="img/sp_xgb_2.png" />
              <p class="fig-label">
                Figure 12 Code for the hyperparameter tuning of the XGB model.
              </p>
              <p>Normalisation improved the performance of the XGB model.</p>
              <p>
                The LSTM-RNN was built using Pytorch. A function was created to
                display the data as tensors, see Figure 13. As feature scales
                can affect neural networks, functions were created to accept
                tensors, normalise or rescale the data, and return the data to
                the original tensor structure, see Figure 14.
              </p>
              <img class="code-img" src="img/sp_dl_1.png" />
              <p class="fig-label">
                Figure 13 Code for the function to create the tensors.
              </p>

              <img class="code-img" src="img/sp_dl_2.png" />
              <p class="fig-label">
                Figure 14 Code for the functions to normalise and rescale the
                data and return to the original tensor structure.
              </p>
              <p>
                The network was created using object-oriented programming, see
                Figure 15.
              </p>
              <img class="code-img" src="img/sp_dl_3.png" />
              <p class="fig-label">
                Figure 15 Code building the LSTM-RNN using object-oriented
                programming.
              </p>
              <p>
                NeuralNetRegressor from the skorch library and GridSearchCV from
                the scikit-learn library were used for hyperparameter tuning of
                the LSTM-RNN, see Figure 16.
              </p>
              <img class="code-img" src="img/sp_dl_4.png" />
              <p class="fig-label">
                Figure 16 Hyperparameter tuning of the LSTM-RNN.
              </p>
              <p>
                Dataloaders were created to allow the data to be fed to the
                network. The training loop was performed with MSEloss as the
                criterion and Adam optimiser, Figure 17.
              </p>
              <img class="code-img" src="img/sp_dl_5.png" />
              <p class="fig-label">Figure 17 Code for the training loop.</p>
              <p>
                The evaluation loop was then run with the training, Figure 18,
                and testing, Figure 19, tensors.
              </p>
              <img class="code-img" src="img/sp_dl_6.png" />
              <p class="fig-label">
                Figure 18 Code for the evaluation loop with the training set.
              </p>
              <img class="code-img" src="img/sp_dl_7.png" />
              <p class="fig-label">
                Figure 19 Code for the evaluation loop with the testing set.
              </p>
              <p class="modal-subheading">
                Use predictions for twelve months to compare results using plots
                and performance metrics.
              </p>
              <p>
                The models were used to make predictions for the twelve-month
                period at the end of the dataset. The predictions from the
                models were compared using three performance metrics: RMSE, MAE
                and R2, the results of these are show in Table 2. The LSTM-RNN
                produced the best values for the RMSE and MAE and the
                second-best value for the R2, with the SARIMAX II model
                producing the best R2 result. The model predictions were also
                compared using visualisations. Figure 20 shows each of the
                predictions made by the models plotted next to the true power
                output values for that period. Both SARIMAX models and XGB model
                without normalisation have a similar pattern to their
                predictions that mimics the shape of the true values but
                overestimates the power outputs for the 04/23 ‚Äì 06/23 and the
                08/23 months. Normalisation appears to stop the XGB model
                overestimating the power outputs. This model gives close
                estimates of power output up to the 06/23, then starts to
                underestimate. The LSTM-RNN model produces a curve to estimate
                the power output, following the pattern of the true values less
                closely than the other models, but gives more accurate
                predictions overall. Figure 21 shows linear regression for the
                predicted monthly power outputs compared against the true
                outputs. The R2 values, Table 2, correspond to these. The more
                accurate the model predictions, the closer the points lie to the
                line. Models appear to make better predictions when the true
                power output is low, these low power output values correspond to
                the winter months. However, the winter months for the test set
                lie the closest in time to the end of the training set, which
                may be causing there more accurate predictions. Future work
                could use different length horizon times for a model to evaluate
                whether winter months or values closer to the end of the
                training set are predicted more accurately.
              </p>
              <img class="tab" src="img/metrics_tab.png" />
              <p class="fig-label">
                Table 2 The performance metrics used for model evaluation. RMSE
                ‚Äì root mean squared error. MAE ‚Äì mean absolute error.
              </p>
              <img class="bigger-img" src="img/truth_pred_plot.png" />
              <p class="fig-label">
                Figure 20 A plot of the true power outputs for the 12-month
                testing dataset alongside the predictions made by each model.
              </p>
              <img class="bigger-img" src="img/lr.png" />
              <p class="fig-label">
                Figure 21 Plots of the linear regression performed for each
                model‚Äôs predictions against the true power output values.
              </p>

              <p class="modal-subheading">References</p>

              <p class="references">
                Mellit, A., Massi Pavan, A., Ogliari, E., Leva, S. and Lughi, V.
                (2020) Advanced Methods for Photovoltaic Output Power
                Forecasting: A Review. Applied Sciences [online]. 10 (2), p.
                487. Available from: https://www.mdpi.com/2076-3417/10/2/487
                [Accessed 1 November 2023].
              </p>
              <hr />
              <p class="gh-txt">
                The full report, code and data can be found on GitHub:
              </p>
              <a
                href="https://github.com/AliWilkinson/Solar_panel_power_forecasting/tree/main"
                target="_blank"
                title="Github"
                class="gt-project-link"
              >
                <i class="fab fa-github"></i>
              </a>
            </div>
          </div>
        </div>
      </div>

      <div class="col-md-4 projects">
        <a href="#portfolio" onclick="openModal('myModal2')">
          <h4>Bristol Air Data Management</h4>
          <img class="project-img" src="img/pollution-er.png" />
        </a>
      </div>

      <div id="myModal2" class="modal">
        <!-- Modal content -->
        <div class="modal-content">
          <span class="close" onclick="closeModal('myModal2')">&times;</span>
          <!-- Data managment project -->
          <div class="modal-scrollable-content">
            <h4>Data Management Project using Bristol Air Quality Data</h4>
            <p class="gh-txt">Code and data can be found on GitHub:</p>
            <a
              href="https://github.com/AliWilkinson/Pollution_Data_management_project/tree/main"
              target="_blank"
              title="Github"
              class="gt-project-link"
            >
              <i class="fab fa-github"></i>
            </a>
            <p class="skills">
              This project demonstrates my skills in data management techniques:
              ER models; relational databases; SQL; using Python to create,
              connect and populate a SQL database; NoSQL databases; MongoDB and
              JSON files.
            </p>
            <hr />
            <div class="project-content">
              <p>
                The dataset contains information from different air sensors
                located around Bristol. It includes information on the sensors,
                such as name, location and coordinates as well as data on the
                levels of pollutants detected by the sensors.
              </p>
              <p class="modal-subheading">Relational Databases</p>
              <p>
                An entity-relationship model was created to model the
                relationships within the dataset. The Schema entity describes
                the units of measurement for Readings entity. The diagram shows
                the one-to-many relationship between the Site and Readings
                entities with primary keys as Site_ID and Reading_ID. Site_ID is
                the foreign key in the Readings entity.
              </p>
              <img class="project-img" src="img/pollution-er.png" />
              <p class="fig-label">ER diagram</p>
              <p>
                The dataset was
                <a
                  href="https://github.com/AliWilkinson/Pollution_Data_management_project/blob/main/crop.py"
                  target="_blank"
                  title="GitHub crop.py"
                  >cropped</a
                >
                for values earlier than 2010 and
                <a
                  href="https://github.com/AliWilkinson/Pollution_Data_management_project/blob/main/clean.py"
                  target="_blank"
                  title="GitHub clean.py"
                  >cleaned</a
                >
                for mismatching site entries.
              </p>
              <p>
                A
                <a
                  href="https://github.com/AliWilkinson/Pollution_Data_management_project/blob/main/populate.py"
                  target="_blank"
                  title="GitHub populate.py"
                  >python file</a
                >
                was written to create and populate a relational database.
              </p>
              <p>SQL queries were written and executed from this database:</p>
              <div class="row">
                <div class="col-md-4">
                  <img class="sql-img" src="img/query-a-answer.png" />
                </div>
                <div class="col-md-4">
                  <img class="sql-img" src="img/query-b-answer.png" />
                </div>
                <div class="col-md-4">
                  <img class="sql-img" src="img/query-c-answer.png" />
                </div>
              </div>
              <p class="fig-label">SQL queries</p>
              <p>
                A
                <a
                  href="https://github.com/AliWilkinson/Pollution_Data_management_project/blob/main/insert-100.py"
                  target="_blank"
                  title="GitHub insert-100"
                  >python file</a
                >
                was also written to create SQL queries to insert the first 100
                rows into the Readings entity.
              </p>
              <p class="modal-subheading">NoSQL Databases</p>
              <p>
                One site was taken from the relational database to be modelled
                to a NoSQL database. A key-value approach was used, and the data
                stored as a JSON file before being imported to a MongoDB
                database.
              </p>
              <div class="row">
                <div class="col-md-6">
                  <img class="json-img" src="img/createJSONimg.png" />
                  <p class="fig-label">Python code to create JSON file</p>
                </div>
                <div class="col-md-6">
                  <img class="json-img" src="img/JSONfilesnip.png" />
                  <p class="fig-label">JSON file snippet</p>
                </div>
              </div>
              <p>
                The data is organised in a denormalized model with embedded
                documents due to the one-to-many relationship of one site to
                many readings. One document contains the data for the site which
                includes the site id, site name and its geo location. Inside
                this, the document for the readings is embedded. The key
                readings contains an array for each reading for the site. This
                document was placed inside one collection called sites within
                the database. This approach was used over having one document
                for each reading and embedding the site data into each of these
                as this would cause much repetition of the site data. The
                denormalized model with embedded documents was used over the
                normalised model due to faster querying. As only one collection
                needs to be queried, results are returned faster. Also, with the
                normalised model, the documents for each site would contain
                large, potentially growing arrays of reading ids.
              </p>
              <p>
                The database was queried to return the readings for the year
                2023 sorted by the concentration of oxides of nitrogen (NOx)
                from highest to lowest:
              </p>
              <img src="img/mdb-q.jpg" class="json-img" />
              <p class="fig-label">MongoDB query</p>
              <img src="img/mongoDBquery.png" class="json-img" />
              <p class="fig-label">Query result</p>
              <p>
                The full NoSQL report can be found on
                <a
                  href="https://github.com/AliWilkinson/Pollution_Data_management_project/blob/main/nosql.md"
                  target="_blank"
                  title="GitHub NoSQL"
                  >GitHub.</a
                >
              </p>
              <hr />
              <p class="gh-txt">Code and data can be found on GitHub:</p>
              <a
                href="https://github.com/AliWilkinson/Pollution_Data_management_project/tree/main"
                target="_blank"
                title="Github"
                class="gt-project-link"
              >
                <i class="fab fa-github"></i>
              </a>
            </div>
          </div>
        </div>
      </div>

      <div class="col-md-4 projects">
        <a href="#portfolio" onclick="openModal('myModal3')">
          <h4>Chemical Classification</h4>
          <img class="project-img" src="img/chem_class_proj.jpg" />
        </a>
      </div>

      <div id="myModal3" class="modal">
        <!-- Advanced statistics chemical classification project -->
        <div class="modal-content">
          <span class="close" onclick="closeModal('myModal3')">&times;</span>
          <div class="modal-scrollable-content">
            <h4>
              The development of a statistical model to classfiy chemicals
            </h4>
            <p class="gh-txt">
              The full Quarto report, code and data can be found on GitHub:
            </p>
            <a
              href="https://github.com/AliWilkinson/Chemical_classification_project"
              target="_blank"
              title="Github"
              class="gt-project-link"
            >
              <i class="fab fa-github"></i>
            </a>
            <p class="skills">
              This project demonstrates my skills using R, R packages, R-studio,
              Quarto, data analysis with R and statistical model development
              with R.
            </p>
            <hr />
            <div class="project-content">
              <p>
                The brief for this project was to create a classification tool
                for use by a company doing chemical analysis. They wished to
                know if they need to measure all variables, or if a subset of
                variables could achieve the same out-of-sample classification
                performance. Being able to use a subset of variables would mean
                faster processing times; but they would not wish to sacrifice
                classification performance for the sake of marginal speed gains.
              </p>
              <p>
                The data contained information on different chemicals and their
                classes. There were 20 numerical variables and one categorical
                variable representing the target variable.
              </p>
              <p>
                The report is created as a Quarto document and the exploratory
                data analysis and model development is written in R.
              </p>
              <p class="modal-subheading">
                Data Preparation and Exploratory Data Analysis
              </p>
              <p>
                The data was checked for missingness. Only 2% of the data was
                found to be missing, therefore only complete cases were used in
                EDA and model development.
              </p>
              <p>
                The data was first split into training, test and validation sets
                with a split of 50% training, 25% test and 25% validation.
              </p>
              <p>
                The training set was examined to view outliers; the distribution
                of the whole training set; distribution of samples between
                classes; the distribution of each variable for each class and
                descriptive statistics for each variable. Figures for these can
                be found in the
                <a
                  href="https://github.com/AliWilkinson/Chemical_classification_project/blob/main/scripts/Chem_analysis.pdf"
                  target="_blank"
                >
                  report on the GitHub repository.
                </a>
              </p>
              <p>
                A correlation plot was created to view correlations between the
                variables, Figure 1.
              </p>
              <img class="modal-img" src="img/chem_class_proj.jpg" />
              <p class="fig-label">Figure 1</p>
              <p>
                Variable V14 with V13 and V17 appear to be strongly negatively
                correlated. Variable V17 and V15 also appear to be strongly
                negatively correlated. Variables V14 and V15 appear to have
                moderate positive correlation with each other. This means that
                there are possible relationships between the variables and means
                the dimensionality of the dataset can potentially be reduced
                using Principal Component Analysis (PCA).
              </p>
              <p class="modal-subheading">
                Principal Components Analysis (PCA)
              </p>
              <p>
                Performing parallel analysis prior to PCA showed that 5
                dimensions should be retained, as these had eigenvalues greater
                than one. This showed how much variation is being explained more
                than by chance, and so how many dimensions to retain to explain
                this variation. Table 1 shows the linear combination of the
                variables for each principal component created by the PCA. It
                shows the results of all 20 variables captured by 5 dimensions
                showing that we can still capture the same information from the
                observations but with fewer dimensions.
              </p>
              <img class="modal-img" src="img/cc_img2.jpg" />
              <p class="fig-label">Table 1</p>

              <p>
                Principal component 1 is responsible for capturing the largest
                variation in the data. It shows a linear combination of the
                variables. The variables 13, 14, 15 and 17 are captured the most
                by principal component 1. The high negative correlation between
                variables 14 and 17 is captured here by showing that they have
                similar magnitudes in the opposite directions for their column
                normed scores. The high negative correlation between 17 and 15,
                and 13 and 14 are also captured by principal component 1, as
                well as the positive correlation between variables 14 and 15.
                These correlations were shown to be present in the original
                training dataset. This means that principal component 1 can
                represent the results covered by these variables and reduces the
                need for all four raw variables to be in the data. Principal
                component 2 captures variable 5 in one direction and variable 6
                in the other direction. Principal component 3 predominantly
                describes variable 18, and variable 12 to a slightly lesser
                extent in the opposite direction. Principal component 4 captures
                variable 4. It also captures variables 9, 3 and 7 to a lesser
                extent in the opposite direction. Principal component 5
                predominantly captures variable 1; also, variables 2 and 12 in
                the same direction and 11 and 5 in the opposite direction.
              </p>
              <p class="modal-subheading">Classification Models</p>
              <p>
                Five different classification models were trained and tested:
                Random Forest, K-Nearest Neighbours (KNN), Model Based
                Discriminant Analysis (MBDA), Linear Discriminant Analysis (LDA)
                and Quadratic Discriminant Analysis (QDA). Model performance was
                compared using the accuracy metric.
              </p>
              <p>
                Accuracy was chosen as it gives an overall representation of the
                performance of the model. The accuracy is calculated by the
                total number of correct predictions over the total number of
                observations.
              </p>
              <img class="tab" src="img/cc_img3.jpg" />
              <p class="fig-label">Table 2</p>
              <p>
                Table 4 shows the best performing model to be the Random Forest
                model with an accuracy of 86.93% on the in-sample test data.
                This model had a much higher accuracy than the other models. The
                model also had a greater performance on the out-of-sample
                validation set, with an accuracy of 89.89%. This suggests that
                the Random Forest model is not overfitting the training data and
                demonstrates how it performs on unseen data. Due to the nature
                of the algorithm, the Random Forest model was performed on all
                features of the data, without prior PCA. Therefore, for optimal
                classification performance, the findings from this report
                suggest that all variables that were present in the original
                dataset should be measured and included and used in the Random
                Forest model.
              </p>
              <hr />
              <p class="gh-txt">
                The full Quarto report, code and data can be found on GitHub:
              </p>
              <a
                href="https://github.com/AliWilkinson/Chemical_classification_project"
                target="_blank"
                title="Github"
                class="gt-project-link"
              >
                <i class="fab fa-github"></i>
              </a>
            </div>
          </div>
        </div>
      </div>
    </div>
    <br />
    <br />
    <div class="row">
      <div class="col-md-4 projects">
        <a href="#portfolio" onclick="openModal('myModal4')">
          <h4>Obesity Classification</h4>
          <img class="project-img" src="img/ob_img.jpg" />
        </a>
      </div>
      <!-- open up a modal for more info on each project  -->
      <div id="myModal4" class="modal">
        <!-- machine learning obesity classification project -->
        <div class="modal-content">
          <span class="close" onclick="closeModal('myModal4')">&times;</span>
          <div class="modal-scrollable-content">
            <h4>
              Machine learning project building classification models to predict
              obesity category from lifestyle factors
            </h4>
            <p class="gh-txt">
              The full report, code and data can be found on GitHub:
            </p>
            <a
              href="https://github.com/AliWilkinson/Obesity_classification_machine_learning"
              target="_blank"
              title="Github"
              class="gt-project-link"
            >
              <i class="fab fa-github"></i>
            </a>
            <p class="skills">
              This project demonstrates my skills using Python, NumPy, Scipy,
              Pandas, Matplotlib and Seaborn for data analysis and scikit-learn
              for the building, testing and evaluation of machine learning
              multi-class classification models.
            </p>
            <hr />
            <div class="project-content">
              <p>
                This project tests five different machine learning
                classification algorithms using data collected though a web
                survey from individuals in Mexico, Peru and Colombia (Palechor
                and Manotas, 2019).
              </p>
              <p>
                The data contains information on lifestyle factors and a
                category of obesity level calculated from body mass index (BMI)
                and the standards set out by the World Health Organization (WHO)
                and Mexican Normativity (Palechor and Manotas, 2019). The
                original researchers used a SMOTE method (Chawla et al., 2002)
                to balance the classes from the data collected. This resulted in
                77% of the data being synthetically generated. This was
                performed to allow the subsequent development of predictive
                models from this data (Palechor and Manotas, 2019).
              </p>
              <p>
                code for the exploratory data analysis, data preparation for the
                models and model development is written in Python in a Jupyter
                notebook and the libraries: Pandas, NumPy, Matplotlib, Seaborn,
                Scipy, Math and Sklearn are used.
              </p>
              <p class="modal-subheading">Exploratory Data Analysis:</p>
              <p>
                The height and weight values show a moderate correlation of
                0.46, with a p value less than 0.05, showing that this is
                significant. Figure 1 shows the relationship between Height and
                Weight. The regression line shows the linear relationship
                between the two variables
              </p>
              <img class="modal-img" src="img/ob_img1.jpg" />
              <p class="fig-label">Figure 1</p>
              <p>
                The height and weight variables were combined to create the
                variable BMI (body mass index): ùêµùëÄùêº = ùë§ùëíùëñùëî‚Ñéùë° / ‚Ñéùëíùëñùëî‚Ñéùë° 2
              </p>
              <p>
                The boxplot in Figure 2 shows there are no outliers in the
                calculated BMI variable.
              </p>
              <img class="modal-img" src="img/ob_img2.jpg" />
              <p class="fig-label">Figure 2</p>
              <p>
                Figure 3 shows the mean levels of physical activity for each
                target group. The bar chart shows that there is a decreasing
                trend in the levels of physical activity as the level of obesity
                increases.
              </p>
              <img class="modal-img" src="img/ob_img3.jpg" />
              <p class="fig-label">Figure 3</p>
              <p>
                Table 1 is a contingency table showing the frequency of the
                observations in the dataset with a family history of obesity for
                each obesity level. As the obesity level increases, there is an
                increase in the difference between people reporting a family
                history of obesity and those without a family history of
                obesity. The result of the chi-squared test in Table 2 shows
                that there is a significant association between these variables
                as the p-value is less than 0.05.
              </p>

              <img class="tab" src="img/ob_img4.jpg" />
              <p class="fig-label">Table 1</p>
              <img class="tab2" src="img/ob_img5.jpg" />
              <p class="fig-label">Table 2</p>

              <p class="modal-subheading">Data Preparation for the Models:</p>
              <ol>
                <li>
                  The BMI column was dropped due to the categories for the
                  obesity levels being determined by the BMI score. This means
                  the BMI was the same as the target variable in a continuous
                  format.
                </li>
                <li>
                  Categorical features of the data were encoded to allow the
                  data to work with machine learning models. Binary variables
                  were encoded to make the values 0 and 1, ordinal variables
                  were encoded with values to allow the order of the categories
                  to be maintained. The single nominal variable, MTRANS
                  (Transportation used) was encoded using one-hot-encoding to
                  ensure no order was introduced into the categories of this
                  variable.
                </li>
                <li>
                  The data was split into training, test and validation sets
                  with a split of 70%, 15%, 15% respectively.
                </li>
                <li>
                  Normalisation was used to ensure all features were on the same
                  scale to prevent scales affect model performance.
                </li>
              </ol>
              <p class="modal-subheading">Classification Models:</p>
              <p>
                Supervised learning algorithms were chosen for this problem as
                the goal is to classify the observations into one of seven
                categories. Five different algorithms were trained and tested
                for this classification problem. A grid search approach was used
                during hyperparameter tuning for the models, during which a
                five-fold cross validation was performed to analyse the
                performance of the model with each hyperparameter combination.
                The training set was used to train the models and the testing
                set was used to test and tune the hyperparameters. The
                validation set was not touched throughout this process to
                prevent data leakage. This set was saved to evaluate the
                performance of the best model using unseen data.
              </p>
              <p class="modal-subheading">Model Evaluation</p>
              <p>
                The models with the hyperparameters that give the best
                performances were then tested using a ten-fold cross validation
                approach. Figure 4 and Table 3 show a comparison of the accuracy
                scores of the models. The standard deviations of the accuracy
                scores are shown in Table 3. Many of the models performed well
                with an accuracy score over 78%. The adaptive boosting model
                performed poorly compared to the other models with an accuracy
                score of 45%. The best performing model was the Random Forest
                model with an accuracy score of 86%, followed by the Gradient
                Boosting model with an accuracy score of 83%.
              </p>
              <img class="modal-img" src="img/ob_img.jpg" />
              <p class="fig-label">Figure 4</p>
              <img class="tab" src="img/ob_img7.jpg" />
              <p class="fig-label">Table 3</p>
              <p>
                The validation set was used to show the out-of-sample
                performance of the final Random Forest model. Table 4 shows the
                accuracy, weighted F1 score and the area under curve receiver
                operator characteristic (AUC-ROC) for the random forest model
                with the validation set. These scores show that the Random
                Forest model performs well. These scores show that the model is
                not likely to be overfitting the data as the model shows similar
                accuracy performance on the training and validation set
              </p>
              <img class="tab2" src="img/ob_img8.jpg" />
              <p class="fig-label">Table 4</p>
              <p>
                Figure 5 shows the importance of each feature is in the Random
                Forest model. Age appears to be the most important feature,
                followed by frequency of vegetables eaten (FCVC), number of main
                meals (NCP), Time using Technology (TUE), consumption of water
                daily (CH2O) and physical activity frequency (FAF). To further
                develop the model, feature engineering could be performed. For
                example, selecting features based on a feature importance
                threshold to be included in the model, and comparing the
                performance of the model with all the feature to the model with
                the engineered features.
              </p>
              <img class="modal-img" src="img/ob_img9.jpg" />
              <p class="fig-label">Figure 5</p>
              <p class="modal-subheading">References</p>
              <p class="references">
                Chawla, N.V., Bowyer, K.W., Hall, L.O. and Kegelmeyer, W.P.
                (2002) SMOTE: Synthetic Minority Over sampling Technique.
                Journal of Artificial Intelligence Research [online]. 16, pp.
                321‚Äì357. Available from:
                https://www.jair.org/index.php/jair/article/view/10302 [Accessed
                23 August 2023].
              </p>
              <p class="references">
                F.M. and Manotas, A. de la H. (2019) Dataset for estimation of
                obesity levels based on eating habits and physical condition in
                individuals from Colombia, Peru and Mexico. Data in Brief
                [online]. 25, p. 104344. Available from:
                https://www.sciencedirect.com/science/article/pii/S2352340919306985
                [Accessed 23 August 2023].
              </p>
              <hr />
              <p class="gh-txt">
                The full report, code and data can be found on GitHub:
              </p>
              <a
                href="https://github.com/AliWilkinson/Obesity_classification_machine_learning"
                target="_blank"
                title="Github"
                class="gt-project-link"
              >
                <i class="fab fa-github"></i>
              </a>
            </div>
          </div>
        </div>
      </div>
      <div class="col-md-4 projects">
        <a href="#portfolio" onclick="openModal('myModal5')">
          <h4>Learning Analytics</h4>
          <img class="project-img" src="img/la_feature_importance.png" />
        </a>
      </div>
      <div id="myModal5" class="modal">
        <!-- Learning analytics and student success data analysis and classification project -->
        <div class="modal-content">
          <span class="close" onclick="closeModal('myModal5')">&times;</span>
          <div class="modal-scrollable-content">
            <h4>
              Learning Analytics and Student Success: evaluating students
              interactions with a virtual learning environment and predictive
              modelling of student performance.
            </h4>
            <p class="gh-txt">
              The full report, code and data can be found on GitHub:
            </p>
            <a href="" target="_blank" title="Github" class="gt-project-link">
              <i class="fab fa-github"></i>
            </a>
            <p class="skills">
              Group project: collaborated with four other MSc Data Science
              students to develop a project proposal, collect data, analyse
              source data, develop a classification model, and report and
              present the findings.
            </p>
            <p class="skills">
              This project demonstrates my skills using Python for data
              analysis, data pre-processing techniques such as resampling,
              encoding and scaling, and scikit-learn for the building, testing
              and evaluation of machine learning classification models.
            </p>
            <hr />
            <div class="project-content"></div>
          </div>
        </div>
      </div>
      <div class="col-md-4 projects">
        <a href="#portfolio" onclick="openModal('myModal6')">
          <h4>Python Programming without Libraries Challenge</h4>
          <img class="project-img" src="img/pwl_img.png" />
        </a>
      </div>
      <div id="myModal6" class="modal">
        <!-- Python programming without libraries challenge -->
        <div class="modal-content">
          <span class="close" onclick="closeModal('myModal6')">&times;</span>
          <div class="modal-scrollable-content">
            <h4>
              A programming challenge to develop functions from scratch using
              only core Python without additional libraries
            </h4>
            <p class="gh-txt">The code and data can be found on GitHub:</p>
            <a
              href="https://github.com/AliWilkinson/Python_without_libraries"
              target="_blank"
              title="Github"
              class="gt-project-link"
            >
              <i class="fab fa-github"></i>
            </a>
            <p class="skills">
              This project demonstrates my skills with programming fundamentals
              using Python in Jupyter Notebook.
            </p>
            <hr />
            <div class="project-content">
              <p class="modal-subheading">
                Develop a function to find the geometric mean:
              </p>
              <img src="img/pwl_1.png" class="code-img" />
              <p class="modal-subheading">
                Develop a function to read a single column from a CSV file:
              </p>
              <img src="img/pwl_2.png" class="code-img" />
              <p class="modal-subheading">
                Develop a function to read CSV data from a file into memory:
              </p>
              <img src="img/pwl_3.png" class="code-img" />
              <p class="modal-subheading">
                Develop a function to calculate the Spearman‚Äôs Rank Correlation
                Coefficient for two named columns:
              </p>
              <p>
                Develop one function to rank the values and deal with equal
                values.
              </p>
              <img src="img/pwl_4.png" class="code-img" />
              <p>Develop a function to calculate the arithmetic mean.</p>
              <p>
                Develop a function to use these to calculate the Spearman‚Äôs
                Correlation Coefficient.
              </p>
              <img src="img/pwl_5.png" class="code-img" />
              <p class="modal-subheading">
                Develop a function to generate a set of Spearman‚Äôs Rank
                Correlation Coefficients for a given data file:
              </p>
              <img src="img/pwl_6.png" class="code-img" />
              <p class="modal-subheading">
                Develop a function to print a custom table:
              </p>
              <p>
                Take four columns from a dataset, calculate the Spearman‚Äôs
                coefficient, and return a table to display these using a symbol
                to outline the table.
              </p>
              <img class="code-img" src="img/pwl_7.png" />
              <img class="code-img" src="img/pwl_img.png" />
              <hr />
              <p class="gh-txt">The code and data can be found on GitHub:</p>
              <a
                href="https://github.com/AliWilkinson/Python_without_libraries"
                target="_blank"
                title="Github"
                class="gt-project-link"
              >
                <i class="fab fa-github"></i>
              </a>
            </div>
          </div>
        </div>
      </div>
    </div>

    <div id="contact"></div>
    <br />
    <br />
    <section class="contact">
      <h5 class="contact-title">
        You can contact me via email, GitHub or LinkedIn
      </h5>
      <div class="email-icons">
        <a href="mailto:afwilkinson98@gmail.com" class="email-link">
          afwilkinson98@gmail.com
        </a>
        <div class="icons">
          <a
            href="https://github.com/AliWilkinson"
            target="_blank"
            title="Github"
          >
            <i class="fab fa-github"></i>
          </a>
          <a
            href="https://www.linkedin.com/in/afwilkinson/"
            target="_blank"
            title="LinkedIn"
          >
            <i class="fab fa-linkedin"></i>
          </a>
        </div>
      </div>
    </section>

    <p class="small">
      This website was coded by Alexandra Wilkinson, and is
      <a href="https://github.com/AliWilkinson" target="_blank">
        open-sourced!</a
      >
    </p>
    <script src="script.js"></script>
  </body>
</html>
